{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4968711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#  MVSA-Single multimodal experiment (Baseline vs Rigid-Givens)\n",
    "#  - Automatic download from Kaggle\n",
    "#  - Binary sentiment: 0 = negative, 1 = positive\n",
    "#  - Image + text (tweet)\n",
    "#  - Comparison: baseline vs rigid rotation (few Givens) + translation\n",
    "#  - K-fold CV, paired t-test, plots in ./results/\n",
    "# ============================================================\n",
    "\n",
    "# -------- 0) Dependencies (Colab) --------\n",
    "!pip install -q kaggle transformers\n",
    "\n",
    "import os, random, subprocess\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.stats import ttest_rel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.colab import files\n",
    "# from kaggle.api.kaggle_api_extended import KaggleApi  # not used directly, but imports the package\n",
    "\n",
    "\n",
    "# ========================= CONFIG =========================\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    mvsa_root: str = \"/content/data/MVSA_Single\"\n",
    "\n",
    "    num_samples: int | None = None\n",
    "    img_size: int = 224\n",
    "    max_text_len: int = 64\n",
    "\n",
    "    batch_size: int = 3\n",
    "    num_epochs: int = 12\n",
    "    num_folds: int = 2\n",
    "\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    temperature: float = 0.07\n",
    "    lambda_cls: float = 1.0\n",
    "    lambda_ntxent: float = 1.0\n",
    "    lambda_l1: float = 1.0\n",
    "    lambda_cos: float = 1.0\n",
    "\n",
    "    freeze_backbones: bool = True\n",
    "    text_model_name: str = \"prajjwal1/bert-tiny\"\n",
    "\n",
    "    num_givens: int = 16\n",
    "\n",
    "\n",
    "# ========================= UTILS =========================\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def ensure_kaggle_api():\n",
    "    \"\"\"Ensure ~/.kaggle/kaggle.json exists. In Colab, ask user to upload if missing.\"\"\"\n",
    "    kaggle_path = os.path.expanduser(\"~/.kaggle\")\n",
    "    kaggle_json = os.path.join(kaggle_path, \"kaggle.json\")\n",
    "\n",
    "    if not os.path.exists(kaggle_json):\n",
    "        print(\"~/.kaggle/kaggle.json not found.\")\n",
    "        print(\"Please upload your kaggle.json (Kaggle account → Legacy API Key).\")\n",
    "        uploaded = files.upload()\n",
    "        if \"kaggle.json\" not in uploaded:\n",
    "            raise RuntimeError(\"kaggle.json was not uploaded. Run this cell again and select the file.\")\n",
    "        os.makedirs(kaggle_path, exist_ok=True)\n",
    "        os.replace(\"kaggle.json\", kaggle_json)\n",
    "        os.chmod(kaggle_json, 0o600)\n",
    "        print(\"kaggle.json copied to ~/.kaggle/kaggle.json\")\n",
    "    else:\n",
    "        print(\"Found ~/.kaggle/kaggle.json\")\n",
    "\n",
    "\n",
    "def download_mvsa_single_if_needed(cfg: Config):\n",
    "    \"\"\"Download and unzip MVSA-Single from Kaggle if it is not already present.\"\"\"\n",
    "    if os.path.isdir(cfg.mvsa_root) and \\\n",
    "       os.path.isdir(os.path.join(cfg.mvsa_root, \"data\")) and \\\n",
    "       os.path.isfile(os.path.join(cfg.mvsa_root, \"labelResultAll.txt\")):\n",
    "        print(f\"MVSA-Single found at: {cfg.mvsa_root}\")\n",
    "        return\n",
    "\n",
    "    print(\"Downloading MVSA-Single from Kaggle (sayan3270/mvsa-single)...\")\n",
    "    ensure_kaggle_api()\n",
    "    data_root = os.path.dirname(cfg.mvsa_root)\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "    cmd_download = [\n",
    "        \"kaggle\", \"datasets\", \"download\",\n",
    "        \"-d\", \"sayan3270/mvsa-single\",\n",
    "        \"-p\", data_root,\n",
    "        \"-q\"\n",
    "    ]\n",
    "    subprocess.run(cmd_download, check=True)\n",
    "    zip_path = os.path.join(data_root, \"mvsa-single.zip\")\n",
    "    if not os.path.isfile(zip_path):\n",
    "        zips = [f for f in os.listdir(data_root) if f.lower().endswith(\".zip\")]\n",
    "        if not zips:\n",
    "            raise RuntimeError(\"Could not find the MVSA-Single zip after download.\")\n",
    "        zip_path = os.path.join(data_root, zips[0])\n",
    "\n",
    "    print(f\"Unzipping {zip_path} ...\")\n",
    "    subprocess.run([\"unzip\", \"-q\", zip_path, \"-d\", data_root], check=True)\n",
    "\n",
    "    if not os.path.isdir(cfg.mvsa_root):\n",
    "        candidates = [\n",
    "            os.path.join(data_root, d) for d in os.listdir(data_root)\n",
    "            if os.path.isdir(os.path.join(data_root, d)) and \"MVSA_Single\" in d\n",
    "        ]\n",
    "        if candidates:\n",
    "            os.rename(candidates[0], cfg.mvsa_root)\n",
    "\n",
    "    if not (os.path.isdir(cfg.mvsa_root) and\n",
    "            os.path.isdir(os.path.join(cfg.mvsa_root, \"data\")) and\n",
    "            os.path.isfile(os.path.join(cfg.mvsa_root, \"labelResultAll.txt\"))):\n",
    "        raise RuntimeError(\n",
    "            \"MVSA_Single structure is not as expected. \"\n",
    "            f\"Check the contents of {data_root}.\"\n",
    "        )\n",
    "\n",
    "    print(f\"MVSA-Single ready at: {cfg.mvsa_root}\")\n",
    "\n",
    "\n",
    "# ========================= DATASET =========================\n",
    "\n",
    "class MVSASingleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    MVSA-Single (Kaggle) → binary classification:\n",
    "      0 = negative, 1 = positive\n",
    "    Neutral / ambiguous examples are removed.\n",
    "    \"\"\"\n",
    "    def __init__(self, mvsa_root, tokenizer, transform=None,\n",
    "                 max_text_len=64, num_samples=3000):\n",
    "        super().__init__()\n",
    "        self.mvsa_root = mvsa_root\n",
    "        self.data_dir = os.path.join(mvsa_root, \"data\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_text_len = max_text_len\n",
    "\n",
    "        if transform is None:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "        self.transform = transform\n",
    "\n",
    "        label_path = os.path.join(mvsa_root, \"labelResultAll.txt\")\n",
    "        print(f\"Loading labels from {label_path} ...\")\n",
    "        df = pd.read_csv(label_path, sep=\"\\t\", header=0)\n",
    "        print(f\"labelResultAll.txt: {df.shape[0]} rows, {df.shape[1]} columns.\")\n",
    "\n",
    "        if df.shape[1] == 2:\n",
    "            df.columns = [\"image_id\", \"annotation\"]\n",
    "        else:\n",
    "            df = df.iloc[:, [0, -1]]\n",
    "            df.columns = [\"image_id\", \"annotation\"]\n",
    "\n",
    "        df[\"image_id\"] = df[\"image_id\"].astype(str).str.strip()\n",
    "        ann = df[\"annotation\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "        simple = {\"positive\", \"negative\", \"neutral\"}\n",
    "        uniq = sorted(ann.unique())\n",
    "\n",
    "        if set(uniq).issubset(simple):\n",
    "            df[\"sentiment\"] = ann\n",
    "        else:\n",
    "            print(\"Annotations are not direct labels; deriving sentiment (positive/negative/neutral).\")\n",
    "\n",
    "            def derive_sentiment(s):\n",
    "                s_low = str(s).lower()\n",
    "                has_pos = \"positive\" in s_low\n",
    "                has_neg = \"negative\" in s_low\n",
    "                has_neu = \"neutral\" in s_low\n",
    "\n",
    "                if (has_pos and has_neg) or (has_neu and not (has_pos or has_neg)):\n",
    "                    return None\n",
    "                if has_pos:\n",
    "                    return \"positive\"\n",
    "                if has_neg:\n",
    "                    return \"negative\"\n",
    "                if has_neu:\n",
    "                    return \"neutral\"\n",
    "                return None\n",
    "\n",
    "            df[\"sentiment\"] = ann.apply(derive_sentiment)\n",
    "\n",
    "        invalid = df[\"sentiment\"].isna().sum()\n",
    "        if invalid > 0:\n",
    "            print(f\"Dropping {invalid} rows with ambiguous annotations.\")\n",
    "            df = df.dropna(subset=[\"sentiment\"]).reset_index(drop=True)\n",
    "\n",
    "        mask = df[\"sentiment\"].isin([\"positive\", \"negative\"])\n",
    "        dropped_neu = (~mask).sum()\n",
    "        if dropped_neu > 0:\n",
    "            print(f\"Dropping {dropped_neu} 'neutral' examples.\")\n",
    "        df = df[mask].reset_index(drop=True)\n",
    "\n",
    "        df[\"label\"] = (df[\"sentiment\"] == \"positive\").astype(np.float32)\n",
    "\n",
    "        def has_files(row):\n",
    "            img_path = os.path.join(self.data_dir, f\"{row['image_id']}.jpg\")\n",
    "            txt_path = os.path.join(self.data_dir, f\"{row['image_id']}.txt\")\n",
    "            return os.path.isfile(img_path) and os.path.isfile(txt_path)\n",
    "\n",
    "        df[\"has_files\"] = df.apply(has_files, axis=1)\n",
    "        missing = (~df[\"has_files\"]).sum()\n",
    "        if missing > 0:\n",
    "            print(f\"Dropping {missing} rows missing .jpg or .txt.\")\n",
    "            df = df[df[\"has_files\"]].reset_index(drop=True)\n",
    "        df = df.drop(columns=[\"has_files\"])\n",
    "\n",
    "        print(\"Label distribution (before subsampling):\")\n",
    "        print(\n",
    "            df[\"label\"].value_counts().rename({\n",
    "                0.0: \"negative (0)\",\n",
    "                1.0: \"positive (1)\"\n",
    "            })\n",
    "        )\n",
    "\n",
    "        if num_samples is not None and num_samples < len(df):\n",
    "            df = df.sample(num_samples, random_state=42).reset_index(drop=True)\n",
    "            print(f\"Randomly taking {num_samples} pairs.\")\n",
    "        else:\n",
    "            print(f\"Using all {len(df)} available pairs.\")\n",
    "\n",
    "        self.df = df\n",
    "        print(f\"Final dataset: {len(self.df)} binary pairs.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_id = row[\"image_id\"]\n",
    "        label = row[\"label\"]\n",
    "\n",
    "        img_path = os.path.join(self.data_dir, f\"{img_id}.jpg\")\n",
    "        txt_path = os.path.join(self.data_dir, f\"{img_id}.txt\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read().strip()\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_text_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = encoded[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)\n",
    "\n",
    "        label_tensor = torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": label_tensor,\n",
    "            \"text\": text,\n",
    "            \"image_id\": img_id,\n",
    "        }\n",
    "\n",
    "\n",
    "# ========================= RIGID GIVENS TRANSFORM =========================\n",
    "\n",
    "class RigidGivensTransform(nn.Module):\n",
    "    \"\"\"\n",
    "    Rigid transform in R^dim:\n",
    "      x' = R x + t\n",
    "    where R is a product of a small number of 2D Givens rotations\n",
    "    (planes (i_k, j_k) with learnable angles) and t is a learnable translation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 128, num_givens: int = 16, seed: int = 0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        max_pairs = dim // 2\n",
    "        self.num_givens = min(num_givens, max_pairs)\n",
    "\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "        perm = torch.randperm(dim, generator=g)\n",
    "\n",
    "        idx_i = perm[0:2 * self.num_givens:2]\n",
    "        idx_j = perm[1:2 * self.num_givens:2]\n",
    "\n",
    "        self.register_buffer(\"idx_i\", idx_i)\n",
    "        self.register_buffer(\"idx_j\", idx_j)\n",
    "\n",
    "        self.theta = nn.Parameter(torch.zeros(self.num_givens))\n",
    "        self.translation = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, dim]\n",
    "        Apply the sequence of 2D rotations in planes (i_k, j_k) and then add a translation.\n",
    "        \"\"\"\n",
    "        y = x\n",
    "\n",
    "        for k in range(self.num_givens):\n",
    "            i = int(self.idx_i[k])\n",
    "            j = int(self.idx_j[k])\n",
    "            angle = self.theta[k]\n",
    "\n",
    "            c = torch.cos(angle)\n",
    "            s = torch.sin(angle)\n",
    "\n",
    "            xi = y[:, i].clone()\n",
    "            xj = y[:, j].clone()\n",
    "\n",
    "            y[:, i] = c * xi - s * xj\n",
    "            y[:, j] = s * xi + c * xj\n",
    "\n",
    "        return y + self.translation\n",
    "\n",
    "\n",
    "# ========================= MODEL =========================\n",
    "\n",
    "class MultimodalNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 text_model_name: str = \"prajjwal1/bert-tiny\",\n",
    "                 use_affine_text: bool = False,\n",
    "                 num_givens: int = 16):\n",
    "        super().__init__()\n",
    "        self.use_affine_text = use_affine_text\n",
    "\n",
    "        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.cnn = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        self.img_fc = nn.Linear(512, 128)\n",
    "\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        hidden_size = self.text_model.config.hidden_size\n",
    "        if hidden_size == 128:\n",
    "            self.text_head = nn.Identity()\n",
    "        else:\n",
    "            self.text_head = nn.Linear(hidden_size, 128)\n",
    "\n",
    "        if self.use_affine_text:\n",
    "            self.affine_text = RigidGivensTransform(dim=128, num_givens=num_givens)\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, 1)\n",
    "\n",
    "    def encode_image(self, pixel_values):\n",
    "        feats = self.cnn(pixel_values)\n",
    "        feats = feats.flatten(1)\n",
    "        img_emb = self.img_fc(feats)\n",
    "        return img_emb\n",
    "\n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        out = self.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        last_hidden = out.last_hidden_state\n",
    "        cls = last_hidden[:, 0, :]\n",
    "        txt_emb = self.text_head(cls)\n",
    "        return txt_emb\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask):\n",
    "        img_emb = self.encode_image(pixel_values)\n",
    "        txt_emb = self.encode_text(input_ids, attention_mask)\n",
    "\n",
    "        if self.use_affine_text:\n",
    "            txt_emb = self.affine_text(txt_emb)\n",
    "\n",
    "        img_proj = self.proj(img_emb)\n",
    "        txt_proj = self.proj(txt_emb)\n",
    "\n",
    "        img_logits = self.classifier(img_proj)\n",
    "        txt_logits = self.classifier(txt_proj)\n",
    "\n",
    "        return {\n",
    "            \"img_proj\": img_proj,\n",
    "            \"txt_proj\": txt_proj,\n",
    "            \"img_logits\": img_logits,\n",
    "            \"txt_logits\": txt_logits,\n",
    "        }\n",
    "\n",
    "\n",
    "# ========================= LOSSES =========================\n",
    "\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def ntxent_loss(z_i, z_t, temperature=0.07):\n",
    "    z_i = F.normalize(z_i, dim=1)\n",
    "    z_t = F.normalize(z_t, dim=1)\n",
    "    logits = z_i @ z_t.T / temperature\n",
    "    targets = torch.arange(z_i.size(0), device=z_i.device)\n",
    "    loss_i2t = F.cross_entropy(logits, targets)\n",
    "    loss_t2i = F.cross_entropy(logits.T, targets)\n",
    "    return (loss_i2t + loss_t2i) / 2.0\n",
    "\n",
    "\n",
    "def multimodal_loss(outputs, labels, cfg: Config):\n",
    "    img_logits = outputs[\"img_logits\"]\n",
    "    txt_logits = outputs[\"txt_logits\"]\n",
    "    z_i = outputs[\"img_proj\"]\n",
    "    z_t = outputs[\"txt_proj\"]\n",
    "\n",
    "    loss_cls_img = bce(img_logits, labels)\n",
    "    loss_cls_txt = bce(txt_logits, labels)\n",
    "    loss_cls = loss_cls_img + loss_cls_txt\n",
    "\n",
    "    loss_ntx = ntxent_loss(z_i, z_t, temperature=cfg.temperature)\n",
    "    loss_l1 = torch.mean(torch.abs(z_i - z_t))\n",
    "    loss_cos = 1.0 - F.cosine_similarity(z_i, z_t, dim=1).mean()\n",
    "\n",
    "    loss = (cfg.lambda_cls * loss_cls +\n",
    "            cfg.lambda_ntxent * loss_ntx +\n",
    "            cfg.lambda_l1 * loss_l1 +\n",
    "            cfg.lambda_cos * loss_cos)\n",
    "    return loss, {\n",
    "        \"loss_total\": loss.item(),\n",
    "        \"loss_cls\": loss_cls.item(),\n",
    "        \"loss_ntx\": loss_ntx.item(),\n",
    "        \"loss_l1\": loss_l1.item(),\n",
    "        \"loss_cos\": loss_cos.item(),\n",
    "    }\n",
    "\n",
    "\n",
    "# ========================= TRAIN / EVAL =========================\n",
    "\n",
    "def freeze_backbones(model: MultimodalNet):\n",
    "    for p in model.cnn.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.text_model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "\n",
    "def get_optimizer(model: nn.Module, cfg: Config):\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    return torch.optim.Adam(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, cfg: Config, epoch, model_name=\"\"):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    print(f\"[{model_name}] Epoch {epoch+1}/{cfg.num_epochs}\")\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        pixel_values = batch[\"pixel_values\"].to(cfg.device)\n",
    "        input_ids = batch[\"input_ids\"].to(cfg.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(cfg.device)\n",
    "        labels = batch[\"labels\"].to(cfg.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values, input_ids, attention_mask)\n",
    "        loss, _ = multimodal_loss(outputs, labels, cfg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if (step + 1) % 40 == 0:\n",
    "            pct = 100 * (step + 1) / len(dataloader)\n",
    "            print(\n",
    "                f\"[{model_name}] Step {step+1}/{len(dataloader)} \"\n",
    "                f\"({pct:.1f}%) | Loss: {total_loss/(step+1):.4f}\"\n",
    "            )\n",
    "\n",
    "    avg = total_loss / len(dataloader)\n",
    "    print(f\"[{model_name}] Epoch {epoch+1} finished — mean loss: {avg:.4f}\")\n",
    "    return avg\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, cfg: Config):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_img_logits = []\n",
    "    all_txt_logits = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(cfg.device)\n",
    "        input_ids = batch[\"input_ids\"].to(cfg.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(cfg.device)\n",
    "        labels = batch[\"labels\"].to(cfg.device)\n",
    "\n",
    "        outputs = model(pixel_values, input_ids, attention_mask)\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_img_logits.append(outputs[\"img_logits\"].cpu().numpy())\n",
    "        all_txt_logits.append(outputs[\"txt_logits\"].cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(all_labels, axis=0).flatten()\n",
    "    img_probs = torch.sigmoid(torch.tensor(np.concatenate(all_img_logits))).numpy().flatten()\n",
    "    txt_probs = torch.sigmoid(torch.tensor(np.concatenate(all_txt_logits))).numpy().flatten()\n",
    "    y_img = (img_probs > 0.5).astype(int)\n",
    "    y_txt = (txt_probs > 0.5).astype(int)\n",
    "\n",
    "    f1_img = f1_score(y_true, y_img)\n",
    "    f1_txt = f1_score(y_true, y_txt)\n",
    "    f1_mean = 0.5 * (f1_img + f1_txt)\n",
    "    return {\"f1_img\": f1_img, \"f1_txt\": f1_txt, \"f1_mean\": f1_mean}\n",
    "\n",
    "\n",
    "# ========================= MAIN EXPERIMENT =========================\n",
    "\n",
    "def run_experiment(cfg: Config):\n",
    "    set_seed(cfg.seed)\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "    download_mvsa_single_if_needed(cfg)\n",
    "\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.text_model_name)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((cfg.img_size, cfg.img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    print(\"Loading MVSA-Single dataset...\")\n",
    "    dataset = MVSASingleDataset(\n",
    "        mvsa_root=cfg.mvsa_root,\n",
    "        tokenizer=tokenizer,\n",
    "        transform=transform,\n",
    "        max_text_len=cfg.max_text_len,\n",
    "        num_samples=cfg.num_samples,\n",
    "    )\n",
    "    print(f\"Final dataset: {len(dataset)} samples (binary: 0=neg, 1=pos).\")\n",
    "\n",
    "    indices = np.arange(len(dataset))\n",
    "    kf = KFold(n_splits=cfg.num_folds, shuffle=True, random_state=cfg.seed)\n",
    "\n",
    "    baseline_scores = []\n",
    "    affine_scores = []\n",
    "    all_history = []\n",
    "\n",
    "    fold = 0\n",
    "    for train_idx, val_idx in kf.split(indices):\n",
    "        fold += 1\n",
    "        print(\"=\"*80)\n",
    "        print(f\"FOLD {fold}/{cfg.num_folds}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            Subset(dataset, train_idx),\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            Subset(dataset, val_idx),\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "        # ---------- BASELINE ----------\n",
    "        print(\"Training BASELINE model (no rigid transform)...\")\n",
    "        model_base = MultimodalNet(\n",
    "            text_model_name=cfg.text_model_name,\n",
    "            use_affine_text=False,\n",
    "            num_givens=cfg.num_givens\n",
    "        ).to(cfg.device)\n",
    "        if cfg.freeze_backbones:\n",
    "            freeze_backbones(model_base)\n",
    "        optimizer_base = get_optimizer(model_base, cfg)\n",
    "\n",
    "        loss_history_base = []\n",
    "        for epoch in range(cfg.num_epochs):\n",
    "            l = train_one_epoch(model_base, train_loader, optimizer_base, cfg, epoch, \"Baseline\")\n",
    "            loss_history_base.append(l)\n",
    "        metrics_base = evaluate(model_base, val_loader, cfg)\n",
    "        baseline_scores.append(metrics_base[\"f1_mean\"])\n",
    "        print(\n",
    "            f\"BASELINE FOLD {fold} — \"\n",
    "            f\"F1_img={metrics_base['f1_img']:.4f}, \"\n",
    "            f\"F1_txt={metrics_base['f1_txt']:.4f}, \"\n",
    "            f\"F1_mean={metrics_base['f1_mean']:.4f}\"\n",
    "        )\n",
    "\n",
    "        # ---------- RIGID-GIVENS ----------\n",
    "        print(\"Training RIGID-GIVENS model (learnable text rotation+translation)...\")\n",
    "        model_aff = MultimodalNet(\n",
    "            text_model_name=cfg.text_model_name,\n",
    "            use_affine_text=True,\n",
    "            num_givens=cfg.num_givens\n",
    "        ).to(cfg.device)\n",
    "        if cfg.freeze_backbones:\n",
    "            freeze_backbones(model_aff)\n",
    "        optimizer_aff = get_optimizer(model_aff, cfg)\n",
    "\n",
    "        loss_history_aff = []\n",
    "        for epoch in range(cfg.num_epochs):\n",
    "            l = train_one_epoch(model_aff, train_loader, optimizer_aff, cfg, epoch, \"Rigid-Givens\")\n",
    "            loss_history_aff.append(l)\n",
    "        metrics_aff = evaluate(model_aff, val_loader, cfg)\n",
    "        affine_scores.append(metrics_aff[\"f1_mean\"])\n",
    "        print(\n",
    "            f\"RIGID-GIVENS FOLD {fold} — \"\n",
    "            f\"F1_img={metrics_aff['f1_img']:.4f}, \"\n",
    "            f\"F1_txt={metrics_aff['f1_txt']:.4f}, \"\n",
    "            f\"F1_mean={metrics_aff['f1_mean']:.4f}\"\n",
    "        )\n",
    "\n",
    "        all_history.append({\n",
    "            \"fold\": fold,\n",
    "            \"loss_base\": loss_history_base,\n",
    "            \"loss_aff\": loss_history_aff,\n",
    "            \"f1_base\": metrics_base,\n",
    "            \"f1_aff\": metrics_aff\n",
    "        })\n",
    "\n",
    "    # ===================== SUMMARY =====================\n",
    "\n",
    "    baseline_scores = np.array(baseline_scores)\n",
    "    affine_scores = np.array(affine_scores)\n",
    "    diffs = affine_scores - baseline_scores\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"CROSS-VALIDATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    for i, (b, a) in enumerate(zip(baseline_scores, affine_scores), start=1):\n",
    "        print(f\"Fold {i}:  Baseline F1={b:.4f} | Rigid-Givens F1={a:.4f} | Δ={a-b:.4f}\")\n",
    "\n",
    "    mean_base, std_base = baseline_scores.mean(), baseline_scores.std(ddof=1)\n",
    "    mean_aff, std_aff = affine_scores.mean(), affine_scores.std(ddof=1)\n",
    "\n",
    "    print(\"\\nMean ± std F1 over folds:\")\n",
    "    print(f\"  Baseline     : {mean_base:.4f} ± {std_base:.4f}\")\n",
    "    print(f\"  Rigid-Givens : {mean_aff:.4f} ± {std_aff:.4f}\")\n",
    "    print(f\"  Improvement (Rigid-Givens - Baseline): {diffs.mean():.4f}\")\n",
    "\n",
    "    t_stat, p_value = ttest_rel(affine_scores, baseline_scores)\n",
    "    print(\"\\nPaired t-test (Rigid-Givens vs Baseline, F1_mean)\")\n",
    "    print(f\"  t = {t_stat:.4f},  p = {p_value:.6f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"   Difference is statistically significant at α = 0.05.\")\n",
    "    else:\n",
    "        print(\"   Difference is NOT statistically significant at α = 0.05.\")\n",
    "\n",
    "    np.save(\"results/baseline_f1.npy\", baseline_scores)\n",
    "    np.save(\"results/rigid_givens_f1.npy\", affine_scores)\n",
    "\n",
    "    summary_path = os.path.join(\"results\", \"summary_mvsa_rigid_givens.txt\")\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        f.write(\"MVSA-Single MULTIMODAL EXPERIMENT (binary pos/neg)\\n\")\n",
    "        f.write(\"Baseline vs Rigid-Givens (few Givens rotations + translation)\\n\")\n",
    "        f.write(\"===============================================================\\n\\n\")\n",
    "        for k, v in cfg.__dict__.items():\n",
    "            f.write(f\"{k}: {v}\\n\")\n",
    "        f.write(\"\\nF1 per fold (mean of img+txt):\\n\")\n",
    "        for i, (b, a) in enumerate(zip(baseline_scores, affine_scores), start=1):\n",
    "            f.write(f\"  Fold {i}: baseline={b:.4f}, rigid-givens={a:.4f}, delta={a-b:.4f}\\n\")\n",
    "        f.write(f\"\\nBaseline mean ± std: {mean_base:.4f} ± {std_base:.4f}\\n\")\n",
    "        f.write(f\"Rigid-Givens mean ± std: {mean_aff:.4f} ± {std_aff:.4f}\\n\")\n",
    "        f.write(f\"Delta (rigid-givens - baseline): {diffs.mean():.4f}\\n\")\n",
    "        f.write(f\"Paired t-test: t = {t_stat:.4f}, p = {p_value:.6f}\\n\")\n",
    "\n",
    "    print(f\"\\nSummary saved to {summary_path}\")\n",
    "\n",
    "    # --------- PLOTS ----------\n",
    "    folds = np.arange(1, cfg.num_folds + 1)\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    bar_width = 0.35\n",
    "    plt.bar(folds - bar_width/2, baseline_scores, bar_width, label=\"Baseline\")\n",
    "    plt.bar(folds + bar_width/2, affine_scores, bar_width, label=\"Rigid-Givens\")\n",
    "    plt.xticks(folds)\n",
    "    plt.xlabel(\"Fold\")\n",
    "    plt.ylabel(\"F1 (mean img+txt)\")\n",
    "    plt.title(\"F1 per fold: Baseline vs Rigid-Givens (MVSA-Single)\")\n",
    "    plt.legend()\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.savefig(\"results/f1_per_fold.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    plt.boxplot([baseline_scores, affine_scores],\n",
    "                labels=[\"Baseline\", \"Rigid-Givens\"])\n",
    "    plt.ylabel(\"F1 (mean img+txt)\")\n",
    "    plt.title(\"F1 distribution across folds (MVSA-Single)\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.savefig(\"results/f1_boxplot.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    for hist in all_history:\n",
    "        fd = hist[\"fold\"]\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(hist[\"loss_base\"], label=\"Baseline\")\n",
    "        plt.plot(hist[\"loss_aff\"], label=\"Rigid-Givens\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Training loss\")\n",
    "        plt.title(f\"Training loss — Fold {fd}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f\"results/loss_fold{fd}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "    print(\"\\nExperiment finished. Metrics, plots and arrays are saved in ./results/\")\n",
    "\n",
    "\n",
    "# ========================= ENTRY POINT =========================\n",
    "\n",
    "cfg = Config()\n",
    "print(\"CONFIG:\")\n",
    "for k, v in cfg.__dict__.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "run_experiment(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
